import { Callout } from "nextra-theme-docs";

# Benchmarks

Aiken has built-in support for benchmarking through a syntax similar to [property-based tests](/language-tour/tests#property-based-test). Benchmarks allow you to measure execution costs (memory and CPU) across increasing input complexity.

## Writing benchmarks

### Defining samplers

To write a benchmark, use the `bench{:ak}` keyword along with a `Sampler{:ak}`. A `Sampler{:ak}` takes a complexity parameter and generates increasingly complex inputs based on a specified growth pattern. `Sampler{:ak}`s can be constructed using the [`aiken-lang/sample`](https://github.com/aiken-lang/sample) package

```aiken
use aiken/sample.{Linear}

fn my_function(n: Int) -> Int {
  n * 2
}

bench multiply_bench(n via sample.int(Linear(5))) {
  my_function(n)
}
```

A `Sampler{:ak}` is introduced as a special annotation for the argument using the `via` keyword. The sample library uses a `Growth{:ak}` to control how input complexity scales:

- `Constant{:ak}` - Fixed complexity
- `Linear(base){:ak}` - Linear growth: `f(n) = base * n`
- `Exponential(base){:ak}` - Exponential growth: `f(n) = base^n`
- `Logarithmic(base){:ak}` - Logarithmic growth: `f(n) = log_base(n)`

### Composing samplers

Samplers can be composed to create more complex benchmarking scenarios:

```aiken
use aiken/sample.{Linear}

// Inline
bench list_operation_inline(xs via sample.list(sample.int(Linear(1)), Linear(5))) {
  list.reverse(xs)
}

// Or, by defining a helper function
fn sample_list() -> Sampler<List<Int>> {
  sample.list(sample.int(Linear(1)), Linear(5))
}

bench list_operation_helper(xs via sample_list()) {
  list.reverse(xs)
}
```

<Callout type="info">
Explore more complex building blocks in [the official library documentation](https://aiken-lang.github.io/sample/).
</Callout>

## Running benchmarks

Benchmarks are collected using the `aiken bench` command. They provide a report showing execution costs (in abstract memory and CPU units) across different complexity levels.
This allows you to analyze how your code's performance scales with the chosen input complexity.

### Running specific benchmarks

Like tests, you can run specific benchmarks using the usual command-line options:

```sh
# Run benchmarks in specific module
aiken bench -m "my_module"

# Run specific benchmark
aiken bench -e -m "my_bench"
```

<Callout type="info">
Benchmarks are particularly useful when optimizing validator scripts since they allow you to measure execution costs across different input sizes and complexity levels.
</Callout>

For more information about the testing functionality that benchmarking builds upon, see the [testing documentation](/language-tour/tests).
